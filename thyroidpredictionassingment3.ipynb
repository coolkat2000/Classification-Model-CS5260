{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0LBeGfhhk+tzGzTELc5hL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coolkat2000/Classification-Model-CS5260/blob/main/thyroidpredictionassingment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Thyroid Cancer Prediction\n",
        "\n",
        "This script loads patient data from an Excel file, preprocesses it,\n",
        "splits the data into training and validation sets (75/25 split),\n",
        "and trains a Random Forest classifier to predict thyroid cancer.\n",
        "\n",
        "### Why Random Forest Classifier?\n",
        "The Random Forest classifier was chosen for the following reasons:\n",
        "- **Robustness & Accuracy**: Random Forest is an ensemble method that reduces overfitting and improves predictive accuracy.\n",
        "- **Handles Categorical & Numerical Data Well**: It can effectively work with mixed data types present in the dataset.\n",
        "- **Feature Importance Evaluation**: It provides insights into which features contribute most to the prediction.\n",
        "- **Works Well With Small to Medium-Sized Datasets**: Given the dataset size, Random Forest is computationally efficient.\n",
        "- **Handles Missing Values & Noisy Data**: Since it averages multiple decision trees, it's less affected by missing or noisy values.\n",
        "\n",
        "Author: Micky Simwenyi"
      ],
      "metadata": {
        "id": "jmV0lGLXoUpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading all the required Libararies"
      ],
      "metadata": {
        "id": "qc32y-BUoopq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "gYdFamTKoy7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload file manually in Google Colab\n",
        "The below will prompt you to load the dataset in Colab for processing and model buiding\n"
      ],
      "metadata": {
        "id": "izI0Lnm7rOXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload the thyroid_cancer_risk_data.xlsx file.\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Yj4FIvp-r3jY",
        "outputId": "0d3d9b0d-4cd6-480e-f701-64c68a9f4341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the thyroid_cancer_risk_data.xlsx file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5def7f84-bc50-40ea-b0b9-a929f8035798\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5def7f84-bc50-40ea-b0b9-a929f8035798\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving thyroid_cancer_risk_data.xlsx to thyroid_cancer_risk_data (1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the dataset from an uploaded Excel file."
      ],
      "metadata": {
        "id": "XASki04BuOG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    return pd.read_excel(file_path, sheet_name=\"thyroid_cancer_risk_data\")"
      ],
      "metadata": {
        "id": "CMPfQrMcukqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below Preprocess the dataset by encoding categorical variables and normalizing numerical features.\n",
        "    \n",
        "*   Drops the `Patient_ID` column as it is non-informative.\n",
        "*   Encodes categorical features using Label Encoding.\n",
        "*   Encodes the target variable `Diagnosis`.\n",
        "*   Scales numerical features using StandardScaler."
      ],
      "metadata": {
        "id": "hgWwlr0dwi1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df = df.drop(columns=[\"Patient_ID\"])  # Remove non-informative column\n",
        "\n",
        "    # Encode categorical variables\n",
        "    categorical_cols = [\"Gender\", \"Country\", \"Ethnicity\", \"Family_History\", \"Radiation_Exposure\",\n",
        "                        \"Iodine_Deficiency\", \"Smoking\", \"Obesity\", \"Diabetes\", \"Thyroid_Cancer_Risk\"]\n",
        "\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Encode target variable\n",
        "    target_encoder = LabelEncoder()\n",
        "    df[\"Diagnosis\"] = target_encoder.fit_transform(df[\"Diagnosis\"])\n",
        "\n",
        "    # Normalize numerical features\n",
        "    numerical_cols = [\"Age\", \"TSH_Level\", \"T3_Level\", \"T4_Level\", \"Nodule_Size\"]\n",
        "    scaler = StandardScaler()\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    return df, label_encoders"
      ],
      "metadata": {
        "id": "mlrffhCsxVnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below splits the datset into training (75%) and validation (25%) sets."
      ],
      "metadata": {
        "id": "34lrtGR7y2oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df):\n",
        "    X = df.drop(columns=[\"Diagnosis\"])\n",
        "    y = df[\"Diagnosis\"]\n",
        "    return train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "CMaFrsxszCmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Below trains a Random Forest classifier.\n",
        "    \n",
        "The Random Forest algorithm is an ensemble learning method that combines multiple decision trees.  This reduces overfitting and increases accuracy. The model is trained with 100 estimators (trees) to create a strong classifier by aggregating weak learners."
      ],
      "metadata": {
        "id": "f6jwRIWazL3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZxKFMzakzxt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below evalutes the model using accuracy, confusion matrix, and classification report\n",
        "\n",
        "#prints\n",
        "*   Model Accuracy\n",
        "*   Confusion Matrix\n",
        "*   Classification Report\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sAdOSZTJz8Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)"
      ],
      "metadata": {
        "id": "PWt7O-Gr1cKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Main Function to run the entire pipeline in Google Colab\n"
      ],
      "metadata": {
        "id": "SQncUBdP3kRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the entire pipeline in Google Colab.\n",
        "\n",
        "    Steps:\n",
        "    1. Upload and load the dataset.\n",
        "    2. Preprocess the data.\n",
        "    3. Split into training and validation sets.\n",
        "    4. Train the Random Forest classifier.\n",
        "    5. Evaluate the trained model.\n",
        "    \"\"\"\n",
        "    df = load_data(file_path)\n",
        "    df, label_encoders = preprocess_data(df)\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "    model = train_model(X_train, y_train)\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWysLiZo2zPW",
        "outputId": "33bf4622-ecdd-40b5-ed92-6bb576c94d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "Confusion Matrix:\n",
            "[[38515  2284]\n",
            " [ 6879  5495]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89     40799\n",
            "           1       0.71      0.44      0.55     12374\n",
            "\n",
            "    accuracy                           0.83     53173\n",
            "   macro avg       0.78      0.69      0.72     53173\n",
            "weighted avg       0.82      0.83      0.81     53173\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "**Thyroid Cancer Prediction Model Performance Report**\n",
        "\n",
        "**1. Introduction**\n",
        "This report evaluates the performance of a **Random Forest Classifier** trained to predict thyroid cancer based on patient data. The evaluation metrics include accuracy, precision, recall, F1-score, and confusion matrix analysis.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Model Performance Overview**\n",
        "\n",
        "- **Accuracy**: **83%**\n",
        "  - The model correctly classified **83%** of the total instances in the dataset.\n",
        "\n",
        "- **Confusion Matrix**:\n",
        "  |                  | Predicted Benign (0) | Predicted Malignant (1) |\n",
        "  |------------------|----------------------|--------------------------|\n",
        "  | **Actual Benign (0)**  | 38,515                 | 2,284                    |\n",
        "  | **Actual Malignant (1)** | 6,879                  | 5,495                    |\n",
        "  \n",
        "  - **True Negatives (TN)**: **38,515** (Correct benign predictions)\n",
        "  - **False Positives (FP)**: **2,284** (Benign cases misclassified as malignant)\n",
        "  - **False Negatives (FN)**: **6,879** (Malignant cases misclassified as benign)\n",
        "  - **True Positives (TP)**: **5,495** (Correct malignant predictions)\n",
        "\n",
        "---\n",
        "\n",
        "**3. Precision, Recall, and F1-Score**\n",
        "\n",
        "| Class | Precision | Recall | F1-Score | Support |\n",
        "|-------|-----------|--------|----------|---------|\n",
        "| **Benign (0)** | **0.85**  | **0.94** | **0.89** | **40,799** |\n",
        "| **Malignant (1)** | **0.71**  | **0.44** | **0.55** | **12,374** |\n",
        "\n",
        "- **Precision**: The model’s ability to correctly predict a class.\n",
        "  - **Benign (0) Precision = 85%** → 85% of predicted benign cases were truly benign.\n",
        "  - **Malignant (1) Precision = 71%** → 71% of predicted malignant cases were truly malignant.\n",
        "\n",
        "- **Recall**: The proportion of actual cases correctly identified.\n",
        "  - **Benign (0) Recall = 94%** → The model detected **94%** of actual benign cases.\n",
        "  - **Malignant (1) Recall = 44%** → The model detected **only 44%** of malignant cases.\n",
        "\n",
        "- **F1-Score**: Harmonic mean of precision and recall.\n",
        "  - **Benign (0) F1-Score = 0.89** (Strong balance between precision and recall)\n",
        "  - **Malignant (1) F1-Score = 0.55** (Lower performance due to low recall)\n",
        "\n",
        "---\n",
        "\n",
        "**4. Overall Model Evaluation**\n",
        "\n",
        "| Metric          | Precision | Recall | F1-Score |\n",
        "|----------------|-----------|--------|----------|\n",
        "| **Macro Avg**  | **0.78**  | **0.69** | **0.72** |\n",
        "| **Weighted Avg** | **0.82**  | **0.83** | **0.81** |\n",
        "\n",
        "- **Macro Average**: Takes the average of precision, recall, and F1-score across both classes.\n",
        "- **Weighted Average**: Adjusts for class imbalance, giving more weight to common classes.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Key Insights & Recommendations**\n",
        "\n",
        "**Strengths:**\n",
        "- High **accuracy (83%)** and strong **performance for benign cases (94% recall)**.\n",
        "- Model achieves **good precision (82%)** overall.\n",
        "\n",
        "**Weaknesses:**\n",
        "- **Low recall (44%) for malignant cases**, meaning **many cancerous cases are being missed**.\n",
        "- **False Negatives (6,879)** could lead to **missed diagnoses**, posing a **serious medical risk**.\n",
        "\n",
        " **Potential Improvements:**\n",
        "1. **Balance the Dataset**:\n",
        "   - Apply **SMOTE (Synthetic Minority Over-sampling Technique)** to improve recall for malignant cases.\n",
        "   - Consider **undersampling benign cases** to reduce class imbalance.\n",
        "\n",
        "2. **Use Alternative Metrics**:\n",
        "   - **ROC-AUC score** to evaluate true model performance in imbalanced datasets.\n",
        "   - **Precision-Recall curves** to assess sensitivity.\n",
        "\n",
        "3. **Hyperparameter Tuning**:\n",
        "   - Adjust **number of trees, max depth, and feature selection** for improved classification.\n",
        "   - Test **Gradient Boosting** or **XGBoost**, which handle imbalanced datasets better.\n",
        "\n",
        "4. **Use Different Model Architectures**:\n",
        "   - Try **Logistic Regression** for interpretability.\n",
        "   - Use **Neural Networks** for enhanced feature learning.\n",
        "\n",
        "---\n",
        "\n",
        "**6. Conclusion**\n",
        "The Random Forest model demonstrated strong predictive capabilities for benign cases, achieving a high recall of 94%. However, its ability to detect malignant cases remains limited, with a recall of only 44%, meaning a substantial number of actual thyroid cancer cases are being misclassified as benign.\n",
        "\n",
        "This high number of false negatives (6,879 cases) represents a significant clinical risk, as undiagnosed thyroid cancer could lead to delayed treatments and worsening patient outcomes. Therefore, future iterations of the model should focus on enhancing sensitivity towards malignant cases, ensuring that high-risk patients receive timely and accurate diagnoses.\n",
        "\n",
        "To address these concerns, balancing the dataset using oversampling techniques like SMOTE, undersampling the majority class, or using cost-sensitive learning should be explored. Additionally, fine-tuning hyperparameters and evaluating alternative algorithms such as Gradient Boosting, XGBoost, or Neural Networks may improve recall without significantly compromising precision.\n",
        "\n",
        "By implementing these enhancements, the model can become a more reliable tool for thyroid cancer screening, aiding in early detection and improving patient outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "KQb6yUNE78SY"
      }
    }
  ]
}